test:
  avg_pool_kernel_size: 100
  conv_kernel_sizes: [3, 3, 3, 3] 
  num_blocks: [1, 1, 1, 1] 
  num_channels: 64 
  shortcut_kernel_sizes: [1, 1, 1, 1] 
  drop: 0 # proportion for dropout 
  squeeze_and_excitation: 1 # True=1, False=0 
  max_epochs: 200
  optim: "sgd" 
  lr_sched: "CosineAnnealingLR"
  momentum: 0.9
  lr: 0.1 
  weight_decay: 0.0005 
  batch_size: 5
  num_workers: 1
  resume_ckpt: 0 # 0 if not resuming, else path to checkpoint  
  data_augmentation: 1 # True=1, False=0 
  data_normalize: 1 # True=1, False=0 
  grad_clip: 0.1 
  weights_init_type: 'xavier_normal'
  lookahead: 1 

# baseline_ResNet_dropout:
#   avg_pool_kernel_size: 4
#   conv_kernel_sizes: [3, 3, 3, 3] 
#   num_blocks: [2, 1, 1, 1] 
#   num_channels: 64
#   shortcut_kernel_sizes: [1, 1, 1, 1] 
#   drop: 0 # proportion for dropout 
#   squeeze_and_excitation: 0 # True=1, False=0 
#   max_epochs: 200
#   optim: "sgd" 
#   lr_sched: "CosineAnnealingLR"
#   momentum: 0.9
#   lr: 0.1 
#   weight_decay: 0.0005 
#   batch_size: 128
#   num_workers: 16
#   resume_ckpt: 0 # 0 if not resuming, else path to checkpoint  
#   data_augmentation: 1 # True=1, False=0 
#   data_normalize: 1 # True=1, False=0 
#   grad_clip: 0.1 


# numchannel_16_dropout:
#   avg_pool_kernel_size: 4
#   conv_kernel_sizes: [3, 3, 3, 3] 
#   num_blocks: [2, 1, 1, 1] 
#   num_channels: 16
#   shortcut_kernel_sizes: [1, 1, 1, 1] 
#   drop: 0 # proportion for dropout 
#   squeeze_and_excitation: 0 # True=1, False=0 
#   max_epochs: 200
#   optim: "sgd" 
#   lr_sched: "CosineAnnealingLR"
#   momentum: 0.9
#   lr: 0.1 
#   weight_decay: 0.0005 
#   batch_size: 128
#   num_workers: 16
#   resume_ckpt: 0 # 0 if not resuming, else path to checkpoint  
#   data_augmentation: 1 # True=1, False=0 
#   data_normalize: 1 # True=1, False=0 
#   grad_clip: 0.1 

# numchannel_32_dropout:
#   avg_pool_kernel_size: 4
#   conv_kernel_sizes: [3, 3, 3, 3] 
#   num_blocks: [2, 1, 1, 1] 
#   num_channels: 32
#   shortcut_kernel_sizes: [1, 1, 1, 1] 
#   drop: 0 # proportion for dropout 
#   squeeze_and_excitation: 0 # True=1, False=0 
#   max_epochs: 200
#   optim: "sgd" 
#   lr_sched: "CosineAnnealingLR"
#   momentum: 0.9
#   lr: 0.1 
#   weight_decay: 0.0005 
#   batch_size: 128
#   num_workers: 16
#   resume_ckpt: 0 # 0 if not resuming, else path to checkpoint  
#   data_augmentation: 1 # True=1, False=0 
#   data_normalize: 1 # True=1, False=0 
#   grad_clip: 0.1 

# numchannel_48_dropout:
#   avg_pool_kernel_size: 4
#   conv_kernel_sizes: [3, 3, 3, 3] 
#   num_blocks: [2, 1, 1, 1] 
#   num_channels: 48
#   shortcut_kernel_sizes: [1, 1, 1, 1] 
#   drop: 0 # proportion for dropout 
#   squeeze_and_excitation: 0 # True=1, False=0 
#   max_epochs: 200
#   optim: "sgd" 
#   lr_sched: "CosineAnnealingLR"
#   momentum: 0.9
#   lr: 0.1 
#   weight_decay: 0.0005 
#   batch_size: 128
#   num_workers: 16
#   resume_ckpt: 0 # 0 if not resuming, else path to checkpoint  
#   data_augmentation: 1 # True=1, False=0 
#   data_normalize: 1 # True=1, False=0 
#   grad_clip: 0.1 

# numchannel_64_dropout:
#   avg_pool_kernel_size: 4
#   conv_kernel_sizes: [3, 3, 3, 3] 
#   num_blocks: [2, 1, 1, 1] 
#   num_channels: 64
#   shortcut_kernel_sizes: [1, 1, 1, 1] 
#   drop: 0 # proportion for dropout 
#   squeeze_and_excitation: 0 # True=1, False=0 
#   max_epochs: 200
#   optim: "sgd" 
#   lr_sched: "CosineAnnealingLR"
#   momentum: 0.9
#   lr: 0.1 
#   weight_decay: 0.0005 
#   batch_size: 128
#   num_workers: 16
#   resume_ckpt: 0 # 0 if not resuming, else path to checkpoint  
#   data_augmentation: 1 # True=1, False=0 
#   data_normalize: 1 # True=1, False=0 
#   grad_clip: 0.1 

# numchannel_128:
#   avg_pool_kernel_size: 4
#   conv_kernel_sizes: [3, 3, 3, 3] 
#   num_blocks: [2, 1, 1, 1] 
#   num_channels: 128
#   shortcut_kernel_sizes: [1, 1, 1, 1] 
#   drop: 0 # proportion for dropout 
#   squeeze_and_excitation: 0 # True=1, False=0 
#   max_epochs: 200
#   optim: "sgd" 
#   lr_sched: "CosineAnnealingLR"
#   momentum: 0.9
#   lr: 0.1 
#   weight_decay: 0.0005 
#   batch_size: 128
#   num_workers: 16
#   resume_ckpt: 0 # 0 if not resuming, else path to checkpoint  
#   data_augmentation: 1 # True=1, False=0 
#   data_normalize: 1 # True=1, False=0 
#   grad_clip: 0.1 


depth_1_1:
  avg_pool_kernel_size: 4
  conv_kernel_sizes: [3] 
  num_blocks: [2] 
  num_channels: 64
  shortcut_kernel_sizes: [1] 
  drop: 0 # proportion for dropout 
  squeeze_and_excitation: 0 # True=1, False=0 
  max_epochs: 200
  optim: "sgd" 
  lr_sched: "CosineAnnealingLR"
  momentum: 0.9
  lr: 0.1 
  weight_decay: 0.0005 
  batch_size: 128
  num_workers: 16
  resume_ckpt: 0 # 0 if not resuming, else path to checkpoint  
  data_augmentation: 1 # True=1, False=0 
  data_normalize: 1 # True=1, False=0 
  grad_clip: 0.1 

depth_1_2:
  avg_pool_kernel_size: 4
  conv_kernel_sizes: [3] 
  num_blocks: [8] 
  num_channels: 64
  shortcut_kernel_sizes: [1] 
  drop: 0 # proportion for dropout 
  squeeze_and_excitation: 0 # True=1, False=0 
  max_epochs: 200
  optim: "sgd" 
  lr_sched: "CosineAnnealingLR"
  momentum: 0.9
  lr: 0.1 
  weight_decay: 0.0005 
  batch_size: 128
  num_workers: 16
  resume_ckpt: 0 # 0 if not resuming, else path to checkpoint  
  data_augmentation: 1 # True=1, False=0 
  data_normalize: 1 # True=1, False=0 
  grad_clip: 0.1 

depth_2_1:
  avg_pool_kernel_size: 4
  conv_kernel_sizes: [3,3] 
  num_blocks: [2,2] 
  num_channels: 64
  shortcut_kernel_sizes: [1,1] 
  drop: 0 # proportion for dropout 
  squeeze_and_excitation: 0 # True=1, False=0 
  max_epochs: 200
  optim: "sgd" 
  lr_sched: "CosineAnnealingLR"
  momentum: 0.9
  lr: 0.1 
  weight_decay: 0.0005 
  batch_size: 128
  num_workers: 16
  resume_ckpt: 0 # 0 if not resuming, else path to checkpoint  
  data_augmentation: 1 # True=1, False=0 
  data_normalize: 1 # True=1, False=0 
  grad_clip: 0.1 


depth_2_2:
  avg_pool_kernel_size: 4
  conv_kernel_sizes: [3,3] 
  num_blocks: [8,8] 
  num_channels: 64
  shortcut_kernel_sizes: [1,1] 
  drop: 0 # proportion for dropout 
  squeeze_and_excitation: 0 # True=1, False=0 
  max_epochs: 200
  optim: "sgd" 
  lr_sched: "CosineAnnealingLR"
  momentum: 0.9
  lr: 0.1 
  weight_decay: 0.0005 
  batch_size: 128
  num_workers: 16
  resume_ckpt: 0 # 0 if not resuming, else path to checkpoint  
  data_augmentation: 1 # True=1, False=0 
  data_normalize: 1 # True=1, False=0 
  grad_clip: 0.1 

depth_2_3:
  avg_pool_kernel_size: 4
  conv_kernel_sizes: [3,3] 
  num_blocks: [16,12] 
  num_channels: 64
  shortcut_kernel_sizes: [1,1] 
  drop: 0 # proportion for dropout 
  squeeze_and_excitation: 0 # True=1, False=0 
  max_epochs: 200
  optim: "sgd" 
  lr_sched: "CosineAnnealingLR"
  momentum: 0.9
  lr: 0.1 
  weight_decay: 0.0005 
  batch_size: 128
  num_workers: 16
  resume_ckpt: 0 # 0 if not resuming, else path to checkpoint  
  data_augmentation: 1 # True=1, False=0 
  data_normalize: 1 # True=1, False=0 
  grad_clip: 0.1 

depth_3_1:
  avg_pool_kernel_size: 4
  conv_kernel_sizes: [3,3,3] 
  num_blocks: [4,4,3] 
  num_channels: 64
  shortcut_kernel_sizes: [1,1,1] 
  drop: 0 # proportion for dropout 
  squeeze_and_excitation: 0 # True=1, False=0 
  max_epochs: 200
  optim: "sgd" 
  lr_sched: "CosineAnnealingLR"
  momentum: 0.9
  lr: 0.1 
  weight_decay: 0.0005 
  batch_size: 128
  num_workers: 16
  resume_ckpt: 0 # 0 if not resuming, else path to checkpoint  
  data_augmentation: 1 # True=1, False=0 
  data_normalize: 1 # True=1, False=0 
  grad_clip: 0.1 

depth_3_2:
  avg_pool_kernel_size: 4
  conv_kernel_sizes: [3,3,3] 
  num_blocks: [2,2,2] 
  num_channels: 64
  shortcut_kernel_sizes: [1,1,1] 
  drop: 0 # proportion for dropout 
  squeeze_and_excitation: 0 # True=1, False=0 
  max_epochs: 200
  optim: "sgd" 
  lr_sched: "CosineAnnealingLR"
  momentum: 0.9
  lr: 0.1 
  weight_decay: 0.0005 
  batch_size: 128
  num_workers: 16
  resume_ckpt: 0 # 0 if not resuming, else path to checkpoint  
  data_augmentation: 1 # True=1, False=0 
  data_normalize: 1 # True=1, False=0 
  grad_clip: 0.1 

# dropout_test:
#   avg_pool_kernel_size: 4
#   conv_kernel_sizes: [3] 
#   num_blocks: [1] 
#   num_channels: 50 
#   shortcut_kernel_sizes: [1] 
#   drop: 0 # proportion for dropout 
#   squeeze_and_excitation: 0 # True=1, False=0 
#   max_epochs: 20 
#   optim: "sgd" 
#   lr_sched: "CosineAnnealingLR"
#   momentum: 0.9
#   lr: 0.1 
#   weight_decay: 0.0005 
#   batch_size: 128
#   num_workers: 16
#   resume_ckpt: 0 # 0 if not resuming, else path to checkpoint  
#   data_augmentation: 1 # True=1, False=0 
#   data_normalize: 1 # True=1, False=0 
#   grad_clip: 0.1 


# seblock_test:
#   avg_pool_kernel_size: 4
#   conv_kernel_sizes: [3, 3, 3, 3] 
#   num_blocks: [2, 2, 2, 2] 
#   num_channels: 32
#   shortcut_kernel_sizes: [1, 1, 1, 1] 
#   drop: 0 # proportion for dropout 
#   squeeze_and_excitation: 1 # True=1, False=0 
#   max_epochs: 20 
#   optim: "sgd" 
#   lr_sched: "CosineAnnealingLR"
#   momentum: 0.9
#   lr: 0.1 
#   weight_decay: 0.0005 
#   batch_size: 128
#   num_workers: 16
#   resume_ckpt: 0 # 0 if not resuming, else path to checkpoint  
#   data_augmentation: 1 # True=1, False=0 
#   data_normalize: 1 # True=1, False=0 
#   grad_clip: 0.1 


